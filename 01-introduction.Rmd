# Introduction
  
## Some Dualisms
  
The type of multivariate analysis (MVA) we discuss in this book is sometimes called *descriptive* or *exploratory*, as opposed to *inferential* or *confirmatory*. It is located somewhere on the line between computational linear algebra and statistics, and it is probably close to data analysis, Big Data, machine learning, knowledge discovery, data mining, business analytics, or whatever other ill-defined label is used for the *mode du jour*. 

In the days of @gifi_B_90 there was a small-scale civil war between the mathematical statistical (confirmatory) approach to MVA and the data analytical (exploratory) approach. This is not a new conflict, because it has its roots in the Pearson-Yule debate (@mackenzie_78). The first shots in modern times were probably fired by @tukey_62, but much additional polemic heat was generated in the 50 years since Tukey's famous paper. In order to stand our ground we were forced to participate, for example with @deleeuw_A_84c, @deleeuw_A_88c, @deleeuw_C_90b.
  
Here is what @gifi_B_90 says, clearly with some intent to provoke.
  
  > The statistical approach starts with a statistical model, usually based on the 
  > multinormal distribution. The model is assumed to be true, and within the model 
  > certain parametric hypotheses are constructed. The remaining free parameters are 
  > estimated and the hypotheses are tested. (@gifi_B_90, p. 19)
  
  > The data analytic approach does not start with a model, but looks for transformations
  > and combinations of the variables with the explicit purpose of representing the data
  > in a simple and comprehensive, and usually graphical, way. (@gifi_B_90, p. 19)
  
Gifi's first chapter, in particular his section 1.5, outlines an approach to statistics that emphasizes *techniques* over *models*. A technique is a map of data into representations of some sort. Representations can be test statistics, confidence intervals, posterior distributions, tables, graphs. *Statistics studies the construction, properties, and performance of techniques*. Models are used to *gauge* techniques, that is to see how they perform on *synthetic data*, which are data described by equations or generated by sampling. Of course models can also be used to *inspire* techniques, but data analysis does not deal with the inspirational phase. The models themselves are a part of the client sciences, not of statistics. One of the key characteristics of a technique is its *stability*, which is studied by using data perturbations of various sorts. Small and unimportant perturbations of the data should lead to small and unimportant changes in the output. A large and
important class of perturbations is based on sampling from a population, leading to sampling distributions, confidence intervals, hypothesis tests, and standard errors. In Gifi's branch of statistics the emphasis shifts from equations to algorithms, and from explanation to prediction. 
  
In related philosophizing @breiman_01 contrasted the *two cultures* of data modeling (98% of statisticians) and algorithmic modeling (2% of statistians), and implored statisticians to spend less time and energy in the first culture and more in the second. 
  
  > Reading a preprint of Gifi’s book (1990) many years ago uncovered a kindred spirit.
  > (@breiman_01, p. 205)
  
This was written some time after the influential paper by @breiman_friedman_85, which introduced the Gifi-like ACE technique for multiple regression. 
  
The emphasis in the data modeling culture is on explanation or information, the emphasis in the algorithmic modeling culture on prediction. There are various ways to present and evaluate this distinction. A good overview, from the philosophy of science point of view, is @shmueli_10. From the lofty heights of Academia we hear
  
  > The two goals in analyzing data which Leo calls prediction and information I prefer to describe as “management” 
  > and “science.” Management seeks *profit*, practical answers (predictions) useful for decision making in the short 
  > run. Science seeks *truth*, fundamental knowledge about nature which provides understanding and control in the 
  > long run. (Parzen, in the discussion of @breiman_01, p. 224)
  
The emphasis on techniques was also shared by @cleveland_14, who proposed a new curriculum for statistics departments with more emphasis on computing with data and tool evaluation. Another early ally was Laurie Davis, see the interesting papers by @davies_95 and @davies_08. 
  
  > The first chapter of Gifi (1990) contains an interesting discussion of statistical 
  > practice with special reference to multivariate data. The point of view taken there,
  > with its emphasis on ‘techniques’, has points of contact with the present paper 
  > where we use Tukey’s nomenclature and refer to ‘procedure’. (@davies_08, p. 192)
  
Of course currently the big discussion is if Data Science is actually statistics under a new name. And, more importantly, who should teach it. And, even more importantly, which department should receive the grant money. Parzen may believe that statisticians seek the Truth, whatever that is, but the current situation in Academia is that there is no truth if you do not consider profit. Statistics departments are typically small, and they feel threatened by gigantic Schools of Engineering looming over them (@asa_14, @yu_14). It is partly a question of scale: there are too many data to fit into statistics departments. Numerous new graduate Data Science programs are popping up, in many cases geared toward management and not so much toward science. Statistics departments are seriously considering changing their names, before the levies break and they are flooded by the inevitable rise of data.
  
We shall not pay much attention any more to these turf and culture wars, because basically they are over. Data analysis, in its multitude of disguises and appearances, is the winner. Classical statistics departments are gone, or on their way out. They may not have changed their name, but their curricula and hiring practices are very different from what they were 20 or even 10 years ago.
  
  > Neither do men put new wine into old bottles: else the bottles break, and the wine runneth out, and the bottles 
  > perish: but they put new wine into new bottles, and both are preserved. (Matthew 9:17)
  
Notwithstanding the monumental changes, inferential statistics remains an important form of stability analysis for data analysis techniques. Probabilistic models are becoming more and more important in many branches of science, and perturbing a probabilistic model is most naturally done by sampling. Thus huge parts of classical statistrucs are preserved, and not surprisingly these are exactly the parts useful in data analysis.
  
## Quantifying Qualitative Data
  
One way of looking at  _Multivariate Analysis with Optimal Scaling_, or *MVAOS*, is as an extension of classical linear multivariate analysis to variables that are binary, ordered, or even unordered categorical. In R terminology, classical MVA techniques can thus be applied if some or all of the variables in the dataframe are *factors*. Categorical variables are *quantified* and numerical variables are *transformed* to optimize the linear or bilinear least squares fit.

Least squares and eigenvalue methods for quantifying multivariate qualitative data were first introduced by @guttman_41, although there were some bivariate predecessors in the work of Pearson, Fisher, Maung, and Hirschfeld (see @deleeuw_A_83b or @gower_90 for a historical overview). In this earlier work the emphasis was often on optimizing quadratic forms, or ratios of quadratic forms, and not so much on least squares, distance geometry, and graphical representations such as _biplots_ (@gower_hand_96, @gower_leroux_gardnerlubbe_15, @gower_leroux_gardnerlubbe_16). They were taken up by, among others, @deleeuw_R_68e, by Benzécri and his students in France (see @cordier_65), and by Hayashi and his students in Japan (see @tanaka_79). Early applications can be found in ecology, following an influential paper by @hill_74. With increasing emphasis on software the role of graphical representations has increased and continues to increase.

In @deleeuw_B_74 a first attempt was made to unify most classical descriptive multivariate techniques using a single least squares loss function and a corresponding _alternating least squares (ALS)_ optimization method. His work then bifurcated to the *ALSOS project*, with Young and Takane at the University of North Carolina Chapell Hill, and the *Gifi project*, at the Department of Data Theory of Leiden University.

The ALSOS project was started in 1973-1974, when De Leeuw was visiting Bell Telephone Labs in Murray Hill. ALSOS stands for Alternating Least Squares with Optimal Scaling. The ALS part of the name was provided by @deleeuw_R_68d and the OS part by @bock_60. At early meetings of the Psychometric Society some members were offended by our use of "Optimal Scaling", because they took it to imply that their methods of scaling were supposedly inferior to ours. But the "optimal" merely refers to optimality in the context of a specific least squares loss function.

Young, De Leeuw, and Takane applied the basic ALS and OS methodology to conjoint analysis, regression, principal component analysis, multidimensional scaling, and factor analysis, producing computer programs (and SAS modules) for each of the techniques. An overview of the project, basically at the end of its lifetime, is in @young_deleeuw_takane_C_80 and @young_81.

The ALSOS project was clearly inspired by the path-breaking work of @kruskal_64a and @kruskal_64b, who designed a general way to turn metric multivariate analysis techniques into non-metric ones. In fact, Kruskal applied the basic methodology developed for multidimensional scaling to linear models in @kruskal_65, and to principal component analysis in @kruskal_shepard_74 (which was actually written around 1965 as well). In parallel developments closely related nonmetric methods were developed by @roskam_68 and by Guttman and Lingoes (see @lingoes_73). 

The Gifi project took its inspiration from Kruskal, but perhaps even more from @guttman_41 (and to a lesser extent from the optimal scaling work of Fisher, see @gower_90). Guttman's quantification method, which later became known as _multiple correspondence analysis_, was merged with linear and nonlinear principal component analysis in the HOMALS/PRINCALS techniques and programs (@deleeuw_vanrijckevorsel_C_80). The MVAOS loss function that was chosen ultimately, for example in the work of @vanderburg_deleeuw_verdegaal_A_88, had been used earlier by @carroll_68 in multi-set canonical correlation analysis of numerical variables.

A project similar to ALSOS/Gifi was ACE, short for *Alternating Conditional Expectations*. The ACE method for regression was introduced by @breiman_friedman_85 and the ACE method for principal component analysis by
@koyak_87. Both techniques use the same ALS block relaxation methods, but instead of projecting on a cone or subspace of possible transformation, they apply a smoother (typically Friedman's supersmoother) to find the
optimal transformation. This implies that the method is intended primarily for continuous variables, and that the convergence properties of the ACE algorithm are more complicated than those of a proper ALS algorithms.

An even more closely related project, by Winsberg and Ramsay, uses the cone of I-splines (integrated B-splines)
to define the optimal transformations. The technique for linear models is in @winsberg_ramsay_80 and the one for 
principal component analysis in @winsberg_ramsay_83. Again, the emphasis on monotonic splines indicates that
continuous variables play a larger role than in the ALSOS or Gifi system.

So generally there have been a number of projects over the last 50 years that differ in detail, but apply basically the same methodology (alternating least squares and optimal scaling) to generalize classical MVA techniques. Some of them emphasize transformation of continuous variables, some emphasize quantification of discrete variables. Some emphasize monotonicity, some smoothness. Usually these projects include techniques for regression and principal component analysis, but in the case of Gifi the various forms of correspondence analysis and canonical analysis are also included.

## Beyond Gifi

The techniques discussed in @gifi_B_90, and implemented in the corresponding computer programs, use a particular least squares loss function and minimize it by alternating least squares algorithms. All techniques use what Gifi calls *meet loss*, which is basically the loss function proposed by @carroll_68 for multiset canonical correlation analysis. Carroll's work was extended in Gifi by using optimal scaling to transform or quantify variables coded with indicators, and to use constraints on the parameters to adapt the basic technique, often called *homogeneity analysis*, to different classical MVA techniques.

There have been various extensions of the classical Gifi repertoire by adding
techniques that do not readily fit into meet loss. Examples are  path analysis (@coolen_deleeuw_R_87), linear dynamic systems (@bijleveld_deleeuw_A_91), and factor analysis (@deleeuw_C_04a). But adding these techniques does not really add up to a new framework.

Somewhat more importantly, @deleeuw_vanrijckevorsel_C_88 discuss various ways to generalize meet loss by using *fuzzy coding*. Transformations are no longer step functions, and coding can be done with fuzzy indicators, such as B-spline bases.
This makes it easier to deal with variables that have many ordered categories. Although this is a substantial generalization the basic framework remains the same.

One of the outgrowths of the Gifi project was the *aspect* approach, first discussed systematically by @deleeuw_C_88b, and implemented in the R package
`aspect` by @mair_deleeuw_A_10. In its original formulation it uses *majorization* to optimize functions defined on the space of correlation matrices, where the correlations are computed over transformed variables, coded by indicators.  Thus we optimize aspects of the correlation matrix over transformations of the variables. The `aspect` software was recently updated to allow for B-spline transformations (@deleeuw_E_15b). Many different aspects were implemented, based on eigenvalues, determinants, multiple correlations, and sums of powers of correlation coefficients. Unformately, aspects defined in terms of canonical correlations, or generalized canonical correlations, were not covered. Thus the range of techniques covered by the `aspect` approach has multiple regression and principal component analysis in common with the range of the Gifi system, but is otherwise disjoint from it.

In @deleeuw_C_04a a particular correlation aspect was singled out that could bridge the gap between the aspect approach and the Gifi approach, provided  *orthoblocks* of transformations were introduced. This is combined with the notion of *copies*, introduced in @deleeuw_C_84c, to design a new class of techniques that encompasses all of Gifi and that brings generalized canonical correlation analysis in the aspect framework. Thus correlation aspects, and the majorization algorithms to optimize them, are now a true generalization of the Gifi system. 


*** This is the system we discuss in this book.

